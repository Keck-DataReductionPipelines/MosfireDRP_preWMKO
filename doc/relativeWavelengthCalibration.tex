\section{Relative Wavelength Calibration}
\label{sec:relativeWavelengthCalibration}

\subsection{Overview}

For this stage, we assume that we have a reasonably accurate guess as to
$p_x, p_y \mapsto \lambda$ --- the task is to optimise this map, up to a
monotonic function of $\lambda$. That is, we try to align the ``rows''
of the $p_x, y$ image with each other, but not with any external
reference. To remove the arbitrary-remapping degree of freedom, we can
fix $\lambda(p_x, {p_y}_0)$ for some ${p_y}_0$, in effect aligning the
rest of the slit image with respect to this image row.

\subsection{Progress}

I implemented the algorithm described below, which optimises an
approximated objective function via a non-linear optimisation routine.
Problems:
\begin{enumerate}
\item By the nature of the approximation, the maximum of the
approximated objective function might be suboptimal. Discovering how
severe this problem is in practice would require further thought and
testing
\item The non-linear optimisation routine is not guaranteed to converge
to the minimum of the (approximated) objective function supplied to
it. Again, determining how much of a problem this is in practice would
require more testing.
\end{enumerate}
Preliminary testing did show some problems (i.e. convergence to a poor
solution) --- however, I did not have time to investigate these fully,
so they could be a result of a bug in the implementation, rather than
a problem with the algorithm itself.

%TODO : talk about whether a slower, but more `guaranteed' method could
%be constructed.

\subsection{Algorithm}

To recap : we want to optimise the map $p_x, p_y \mapsto \lambda$, given
an initial guess, while fixing a specified image row.

For convenience, we define $f_0 (p_x) = \lambda (p_x, {p_y}_0)$, and
$\eta (p_x, p_y) = f_0^{-1} (\lambda (p_x, p_y))$, for some $y_0$
in the centre of the slit we're dealing with. We then proceed to
optimise the map $p_x, p_y \mapsto \eta$, so we are attempting to align
the rows with respect to the distinguished row in its `pixel space'. Our
constraint is then that $p_x, {p_y}_0 \mapsto p_x$.

In order to align the rows with respect to each other, we need an image
which has variation in the horizontal direction, while ideally being
approximately uniform in the vertical direction. Both the sky OH lines
and the calibration lamp lines meet these criteria.

In deciding how to optimise the map, a starting point is to investigate
plausible forms of $P(D|\eta)$, where $D$ is the image data. If we
assume that we have processed our raw count image such that the
intensity should be spatially uniform, then we want to model the image
as being a smooth function of $\eta$ alone, where the scale on which it
should be smooth is determined by the slit width etc. A possible model
of a smooth univariate function is given by a `radial basis function
network' --- that is, a linear combination of evenly spaced basis
elements, each of which is a smooth, localised, symmetrical function.

% TODO : more justification for considering the RBF approach?

Letting $v$ denote the vector of image pixel values, $[\eta_i]$ their $\eta$
values, $w$ their inverse variances (assumed uncorrelated), and $X_{ij}
= g_j (\eta_i)$, where $g_j$ is $j$th basis function (so we aim for $v
\approx X \beta$, where $\beta$ is the vector of amplitudes for the
basis functions), we have (assuming a prior $\beta \sim N (0, \Sigma_\beta)$) % TODO - reference for all this ...
\[
%\log P(v|X) = -\frac{n}{2}\log (2 \pi) - \frac{1}{2} \log |C| - \frac{1}{2}v^T C^{-1} v
\log P(v|X) = \dots - \frac{1}{2}v^T C^{-1} v
\]
where $C = X \Sigma_\beta X^T + W^{-1}$, with $W = {\rm diag}(w)$.
%(TODO --- refer to somewhere explaining why we're going to ignore
%the $\log |C|$ term ...)

Now, evaluating the matrix inversion here would be rather expensive, since
$C$ is $n$ by $n$, where $n$ is the number of pixels in the image of the slit, so
of order $2000 \times 100 = 2 \times 10^5$. In order to optimise this function
numerically, we need to approximate it somehow. 

If we assume that $W^{-1} \gg X \Sigma_\beta X^T$, then
$C^{-1} \approx W - W X \Sigma_\beta X^T W$, so
\[
- v^T C^{-1} v \approx  - v^T W v + v^T W X \Sigma_\beta X^T W v
\]
so the relevant quantity (if we take $\Sigma_\beta = \sigma_\beta^2
I$) is $S = \|X^T W v\|^2$. This approximation corresponds to the
expectation that the parameter vector $\beta$ is very small --- thus
the favouring of larger $X$ values in $\| X^T W v\|^2$. However, since
$S = \sum_i ([g_i(\eta)] \cdot v)^2$ (weighting elided) is a sort of
approximation to the magnitude of the projection of $v$ into the space of basis
function combinations, it does have the desired property of increasing
as $v$ can be better fit by such a combination. Also, we are
not optimising $X$ directly, but instead trying to optimise
$\eta$, with $X_{ij} = g_j (\eta_i)$. As a result, so long as the basis
functions are closely spaced, the magnitude of $X$ should not vary much
with $\eta$, making that issue non-lethal.

The next task is to choose a model for $\eta$, and work out the details
of the scheme to optimise $S$. A convenient model of the $p_x, p_y
\mapsto \eta$ function is a linear combination of separable low-order functions,
$\eta(p_x, p_y) = \sum_{i,j} \alpha_{ij}  L_i(p_x) L_j(p_y)$, with e.g.\
the $L_i$ being Legendre polynomials.

Having the gradient (and Hessian) of the objective function available makes non-linear
optimisation much easier. Let $u = W v$, and let $J = (i,j)$ represent a combined index,
so $\eta = \alpha_J L_J$, $L_J(p_x, p_y) = L_i(p_x)L_j(p_y)$
Similarly, let $p_i = (p_x, p_y)$ be the pixel coordinates corresponding to $v_i$,
and let $\eta_i = \eta(p_i)$. Then we have
\[
S = \sum_b (u_a g_b (\eta_a))^2
\]
and with no summation convention on $c$, 
\[
\frac{\partial S}{\partial \alpha_J} = \sum_c 2 u_a g_b (\eta_a) u_c g_b' (\eta_c) \frac{
\partial \eta_c}{\partial \alpha_J}
= 2 u_a g_b (\eta_a) \sum_c u_c g_b'(\eta_c) L_J(p_c)
\]
and
\begin{align*}
\frac{\partial^2 S}{\partial \alpha_J \partial \alpha_K} = & 2
\left(\sum_a u_a g_b'(\eta_a) L_K (p_a)\right)
\left(\sum_c u_c g_b'(\eta_c) L_J (p_c)\right)
\\
 &+ 2 u_a g_b (\eta_a)
\sum_c u_c g_b''(\eta_c) L_J(p_c) L_K(p_c)
\end{align*}
%
For an iterative optimisation, it is important that these expressions
can be computed efficiently. One way of doing this is to go along the
rows, keeping track of which basis functions have support overlapping
with with current pixel, and accumulating into that basis function's
`slot' for the desired derivative.
