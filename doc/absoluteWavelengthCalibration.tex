\section{Absolute Wavelength Calibration}
\label{sec:absoluteWavelengthCalibration}

\subsection{Overview}

In the relative wavelength calibration section
(\S~\ref{sec:relativeWavelengthCalibration}), we optimised the $p_x, p_y
\mapsto \eta$ map, where $f_0 \circ \eta = \lambda$ for some monotonic
$f_0$. The task of the absolute wavelength calibration is to find the
$f_0$ map.

To do this, we need some knowledge of what a spectrum should look like
in $\lambda$ space. For the sky OH lines, this knowledge comes in two obvious
forms --- typical spectra (either generated by some model, or obtained
from previous experiments), and theoretical wavelength values for
strong lines (similarly for calibration lamp spectra). These give
`global' and `local' information respectively --- the overall structure
of a typical spectrum (basically, the relative separations, and very
approximate relative magnitudes, between lines) enables us to identify
which theoretical features lines in the observed spectrum correspond
to, and we can then use our knowledge of the exact wavelengths these
should be at to calibrate the wavlength accurately. As a result, it
makes sense to perform the absolute wavelength calibration in two
stages --- first a coarse alignment to get features sufficiently close
to the correct theoretical lines, then a fine alignment using the exact
wavelength values of these lines.

%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Coarse alignment}

%Some kind of intro

\subsubsection{Progress}

I have implemented the algorithm described below, which is not very
efficient in terms of accuracy, but seems adequate, and haven't noticed
major issues when running it with suitably close initial guesses. There
is definitely enough information to perform a good fit even with a very
bad initial guess, but this algorithm isn't advanced enough to do so
\dots asking computer vision people about techniques from that field
might be useful along these lines.

\subsubsection{Algorithm}
\label{subsubsec:coarseAlignmentAlgorithm}

% TODO - talk about what exactly the pixel count represents?
The task for this stage: we have a vector of pixel values $(\eta_i,
v_i, w_i)$, with $v_i$ the pixel count and $w_i$ the inverse variance,
and a `noise-free' model spectrum. We don't expect this to match the
sky spectrum entirely --- e.g.\ the relative amplitudes of lines 
likely to change --- but the separations of the lines should be the
same, making it useful. Suppose that this spectrum is defined by a function
$\rho = \rho(\lambda)$, and we are trying to optimise $f : \eta \mapsto
\lambda$. Then, modelling the observed values as $v_i = \alpha + \beta
\rho (f (\eta_i)) + \epsilon_i$, where $\epsilon_i \sim N(0,1/w_i)$, and
the prior on $\alpha$ and $\beta$ is broad, we get
\[
\log P(v|f) = \dots + \frac{1}{2} \frac{(v^T W z)^2}{z^T W z}
\]
where
\[
z = p - \frac{\sum p_i w_i}{\sum w_i}
\]
and $p_i = \rho (f (\eta_i))$.

Since we have lots of pixels ($\sim 10^5$), iteratively optimising $f$
by directly evaluating this likelihood at every new value of $p$ would
be rather expensive. However, suppose that $p = X \beta$, where $\beta$
is some `low-dimensional' parameter vector (e.g. the amplitudes for a
couple of thousand radial basis functions, which is low-dimensional
compared to the total number of pixels in the image).

Then we have
\[
v^T W z = v^T W \left(X \beta - \frac{w^T X \beta}{\sum w_i}\mathbf{1}\right)
= \left(v^T W X - \frac{v^T W \mathbf{1}}{\sum w_i} w^T X\right) \beta
\]
so the term in the brackets can be pre-calculated, and as long as we can
calculate the $\beta$ corresponding to a map $f : \eta \mapsto \lambda$,
we can evaluate this at the cost of a dot product. Likewise,
\[
z^T W z = \left(\beta^T X^T - \frac{w^T X \beta}{\sum w_i} \mathbf{1}^T\right)
W \left(X \beta - \frac{w^T X \beta}{\sum w_i} \mathbf{1}\right)
= \beta^T X^T W X \beta - \frac{(w^T X \beta)^2}{\sum w_i}
\] 
We can pre-evaluate $w^T X$ and $X^T W X$; to make $\beta^T X^T
W X \beta$ faster to evaluate, we can approximate $X^T W X$ as a
$k$-diagonal matrix (e.g.\ tri-diagonal), which will be a good
approximation in the RBF case, and obtain the answer from $k$ dot
products. So, the entire log likelihood can be evaluated via a few dot
products, which makes an iterative optimisation feasible.

With regard to finding the the $\beta$ corresponding to a given $f :
\eta \mapsto \lambda$, we want that $p = \rho (f (\eta)) \approx X
\beta$, with $X$ is the fixed RBF matrix. To a resonable approximation,
we can simply use $\beta_i= \rho (f (\eta_i))$, since $X$ should
simply function as a mild smoothing, and not do anything too bad to a
well-behaved $\rho \circ f$. 
%(TODO : talk more about conditions under which this may not be sensible ...)

% TODO - talk about quality of initial guess?
We now have a method to fairly efficiently evaluate a
hopefully-reasonable objective function, given a map $f : \eta \mapsto
\lambda$. We still need a scheme to iteratively optimise this $f$,
starting from some reasonable guess. Since this is the `coarse'
alignment stage, the most important property of this optimisation is
robustness --- it should reliably obtain a solution accurate enough for
the `fine' alignment to deal with.

The scheme implemented in prototype form uses a linear interpolation (lerp)
representation for $f$, and recursively optimises the control points of this
model. Specifically, starting from $f_0$,
\begin{itemize}
\item Firstly, we perform a coarse search over rigid translations, i.e.
we evaluate $L$ for $f = f_0 + \Delta_\lambda$, with $\Delta_\lambda
\in \{-n \delta_\lambda, \dots , n \delta_\lambda\}$ for some $n$, some
spacing $\delta_\lambda$, and choose the best fit. We could also search
over scalings here as well, if necessary.
\item Then we apply the following procedure for stages $0,1,\dots,m$,
some $m$, where for stage $k$, $f$ is represented by a linear
interpolation between $2^k + 1$ control points, approximately equally
spaced in
$\eta$:
\begin{itemize}
\item For each control point $i$, we optimise $f_i$ (the value of $f$ at
the control point) over a regular grid taking values in some interval between $f_{i-1}$
and $f_{i+1}$.
\end{itemize}
\end{itemize}

N.B.\ --- in this procedure, we actually use use a `pixel-normalised'
$\lambda$ space for convenience; i.e.\ we set our initial guess to be the
identity.

This `coarse-to-fine' procedure is not as accurate as it could be, due
to the use of a regular grid search rather than some more sophisticated
optimisation, but it is simple, predictable and appears to peform
adequately in testing.

%TODO : talk about how to determine spacing of basis functions etc. at some point.

%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Fine alignment}

As mentioned above, this stage relies on the fact that we know (from
observation / basic physics) the precise frequencies at which sky OH lines
(and calibration lamp lines) should occur, and that these frequencies should
not change. Thus, once the coarse alignment has obtained a good enough solution
so that there is no real ambiguity as to which peaks in the spectrum correspond
to which theoretical lines, we can obtain accurate wavelength calibrations on a
per-peak basis.

\subsubsection{Progress}

I have implemented the algorithm described below, which seems to work pretty well
(assuming, of course, a good coarse solution). It incorporates a number of
approximations / ad-hoceries, but testing would be needed to determine whether
these are actually a problem.

\subsubsection{Algorithm}

This stage is not quite so straightforward as it may seem, due to the
fact that the spectrum is `smeared' by the optics and by the finite size
of the pixels. As a result, many features that look like single peaks in
the observed spectrum are in fact a combination of multiple very close
emission lines, and there are also plenty of cases where lines are close
enough that the resulting `smeared' spectrum has a single wider peak, or
(depending on the relative amplitudes, which might vary with time) two
peaks whose wings have significant overlap, making the `disambiguation'
task (assigning each observed peak to its correct emission line) more
difficult.

While it would be possible to fit multiple closely-overlapping peaks
simulataneously, it is simpler (and more efficient) to stick with
fitting only a single peak at a time, and split the task into two
stages. Firstly, we process the theoretical line list, using the known
peak-spread and noise characteristics of the instrument to determine
whether a group of emission lines will appear in the spectrum as a
single well-definied peak --- and if so, what its characteristics will
be --- or whether the ambiguity will be such that we are best off not
using a set of lines as part of the calibration process. Once we have
reduced to this set of amalgamated, hopefully trouble-free peaks, we
can find the centre of each one, and combine these values to produce an
updated $\eta \mapsto \lambda$ function.

\vspace{1em}

{\bf{Peak-grouping}}

\vspace{1em}

The input to the peak-grouping stage is a list $\{(\lambda_i, y_i)\}$ of
the `theoretical' wavelengths $\lambda_i$ (known very accurately) and intensities
$y_i$ (prone to vary significantly with changing conditions) of the emission
lines in the relevant spectral band. We also know the predicted number
density of `observations' (i.e. pixel values) and their noise levels.

We construct a set of `line-groups', each of which is parameterised
by its theoretical height $y$, the most likely central wavelength
$\lambda_c$, and a wavelength interval $[\lambda_0, \lambda_1]$ which
should contain the centre of the peak with `high probability'. These
correspond to sets of emission lines which should combine to form a
peak with a single mode. The `most likely central wavelength' and `high
probability interval' notions used are informal ones --- that is, the
method is slightly ad-hoc, not based on a rigorous analysis of the
relevant probability distributions. The process used to construct the
groupings is
%
\begin{itemize}
\item Firstly, we convert the emission line wavelengths into pixel space
and sort them.
\item Then for each line $p_x, y$ (in ascending $p_x$ order):
\begin{itemize}
\item Consider the current line-group, with parameters $y_g, x_c, [x_0,
x_1]$. If a peak with centre $p_x$, height $y$ (note - all peaks assumed
to be Gaussians of the usual width) would always combine with a peak of height
$y_g$ and centre in $[x_0, x_1]$, then we `add' the peak to the group
(the `adding a peak' process is explained below). 
\item If a peak with centre $p_x$, height $y$ would combine to produce
two well-separated peaks with any peak of height $y_g$, centre in $[x_0,
x_1]$, then we put the $p_x, y$ peak in a new group of its own, which
becomes the current group.
\item Finally, if the peak `merges' for some values in $[x_0, x_1]$ but not for
others, then we add it to the group anyway, but mark the whole group as being
dubious.
\end{itemize}
\end{itemize} 

We are able to process peaks from left to right like this because if
a peak is `swallowed' by a group on its left, then its necessarily
swallowed by any groups closer to it on the left, which being separate
from the earlier group were not swallowed by it (this is fairly obvious
from considering the Gaussians in log space, where they are all rigid
movements of the same quadratic).

% Go through the maths of when peaks combine and when they don't ...

To `add' a line $p_x, y$ to a line-group $y_g, x_c, [x_0, x_1]$, producing
a new line-group $y_g', x_c', [x_0', x_1']$, we set
\[
x_0' = \frac{\alpha x_0 y_g + y p_x / \alpha}{\alpha y_g + y/\alpha}
\quad , \quad
x_1' = \frac{x_1 y_g/\alpha + \alpha y p_x}{y_g/\alpha +\alpha y}
\]
where $\alpha > 1$ is a `typical height variation' factor --- i.e.\ it's
plausible that the peaks could be $\alpha$ times higher / lower than
their theoretical values (at the moment, we set $\alpha = 2$ --- this is
not based on any data). So, $x_0'$ and $x_1'$ are the natural worst-case
estimates in either direction (of course, depending on peak shape and separation,
taking a weighted average may not be entirely accurate as regards the location
of the new peak, but we're making approximations anyway).
We set $x_c' = (x_c y_g + y p_x)/(y_g + y)$, again a simple weighted average,
and let $y_g' = f_g (x_c') + f_l (x_c')$, where $f_g$, $f_l$ are the Gaussians
corresponding to the group and the line.

% Make sure we talk about assessing the measurement error for each peak ...

Once we have obtained our line groups, we need to decide how useful they are
likely to be for fitting purposes. Clearly, the groups marked as dubious
because they contain peaks that may or may not merge are not good as regards
fitting a single Gaussian. For others, we obviously have the uncertainty
summarised by the $[x_0, x_1]$ interval. Also, as discussed elsewhere,
the log evidence for a specific-shape model versus a pure-noise
model, for data vector $d$ and shape vector $p$, is given by $L = (d^T 
\hat p)^2 / (2 \sigma^2)$, where the noise on $d$ is $N(0, \sigma^2)$. 
For our purposes, let $d$ be sampled from a Gaussian of height $y$,
width $\Delta$, with $N$ sample-points per unit $x$,
and let $p$ also be sampled from a Gaussian of width $\Delta$, with
offset $\delta_x$ compared to that for $d$. 
For a sufficiently dense and uniform set of samples, we have 
\[
p^T p \approx N \int_{-\infty}^\infty \left(
e^{-x^2 / (2 \Delta^2)} \right)^2
 dx
= N \Delta \sqrt{2 \pi}
\]
so
\[
d^T \hat p \approx \frac{1}{\sqrt{N \Delta \sqrt{\pi}}} N
\int_{-\infty}^\infty
y \exp\left\{-\frac{x^2}{2 \Delta^2} - \frac{(x-\delta_x)^2}{2 \Delta^2}\right\} dx
\]
\[
= y \sqrt{\frac{N}{\Delta \sqrt{\pi}}} \Delta \sqrt\pi e^{-\delta_x^2 / (4 \Delta^2)}
\]
\[
= y \sqrt{N \Delta \sqrt\pi} e^{-\delta_x^2 / (4 \Delta^2)}
\]
and thus
\[
L = \frac{(d^T \hat p)^2}{2 \sigma^2} \approx 
\frac{\sqrt \pi}{2} \frac{y^2}{\sigma^2} N \Delta  \,
 e^{- \delta_x^2 / (2 \Delta^2)} 
\]
This makes sense dimesionally and intuitively --- $N \Delta$ counts the number
of sample points that `matter' to the fitting, so the log-evidence should be linear
in this, and it is quadratic in the height $y$.

If the factor in front of the exponential is large, then there will be strong
evidence for a peak above the noise --- the second derivative wrt $\delta_x$ at
$\delta_x = 0$ gives
\[
\left.\frac{\partial^2 L}{\partial \delta_x^2} \right|_{\delta_x = 0} = - \frac{\sqrt \pi}{2} \frac{y^2}{\sigma^2}\frac{N}{\Delta} =: - \frac{1}{\sigma_L^2}
\] 
so the posterior can be approximated at its peak by a Gaussian of width
$\sigma_L$, which gives a rough idea of the accuracy of the fitting
process. If the factor is small, then it's not really worth trying to
fit to the peak, which will barely stand out above the noise. After
calculating the peak groups, we compute the estimated accuracy of
fitting for each, and sort them by this for use in the fitting stage.
%taking those groups with the best estimated accuracy.

% TODO : check use of likelihood versus evidence throughout ...

% TODO : talk about mathematics of whether peaks merge or not

\vspace{1em}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

{\bf{Fitting a single peak}}

\vspace{1em}

In keeping with the slightly ad-hoc nature of the `line-group' concept,
we don't attempt to keep track of the probable shape of the resulting
peak, instead assuming that it will be a Guassian of the width given
by the instrument spread function. For the most common cases (that of
very closely spaced and physically-related lines, whose amplitudes will
probably vary together), this assumption is very likely to be good
enough, and even getting the predicted width wrong does not bias the
peak-fitting (as is obvious by symmetry), only making it slightly less
accurate.

Explicitly, we let $\{d_i, \eta_i\}$ be the set of pixel values
with $\eta_i$ in a small interval (of the order of the peak
width) around $f^{-1}(\lambda_0)$. We model $d_i = \alpha + \beta
g(\eta_i) + \epsilon_i$, with $g$ a Guassian of appropriate
width centred at $f^{-1}(\lambda_0) + \Delta\eta$. Then, as in
section~\ref{subsubsec:coarseAlignmentAlgorithm} , for broad priors on
$\alpha$ and $\beta$, the log likelihood is given by
\[
L = P(d|\Delta\eta) = \dots + \frac{1}{2} \frac{(d^T W z)^2}{z^T W z}
\]
where
\[
%z = p - \frac{\mathbf{1}^T W p}{\mathbf{1}^T W \mathbf{1}} \mathbf{1}
z = p - \frac{\sum p_i w_i}{\sum w_i}
\]
and $p_i = g(\eta_i)$ is the shape vector. Since evaluating $L$ consists
of a few dot products of vectors of limited length, we can iteratively
optimise it over $\Delta\eta$ using a standard one-dimensional search
routine.

\vspace{1em}

{\bf{Updating $f : \eta \mapsto \lambda$}}

\vspace{1em}

If the peak-fitting stage identifies a sufficiently large number of
strong peaks to perform accurate fits against, we can update $f^{-1} : \lambda \mapsto
\eta$ based on the $\lambda, \eta$ (known, fitted) pairs obtained.
In fact, the current implementation updates this map every time a new peak
is fitted, by parameterising $f^{-1}$ as a low-order polynomial and using
a Kalman-filter style update method. This operates on the assumption of
uncorrelated Gaussian errors --- if this assumption seemed to be causing
poor fitting, a more sophisticated fitting algorithm that considered all
of the peak fits simulataneously could be implemented.

